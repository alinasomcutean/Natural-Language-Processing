# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15gCevrms7GB27JQekMDMW2sRz2uEE_g1
"""

# Code to read file into colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

test = drive.CreateFile({'id':'1R3WYGNIMcvl8Cpmc3Wv6dXQqlwUDu3CL'})
test.GetContentFile('test.ft.txt') 
train = drive.CreateFile({'id':'1_ZeSk5nr4O0mpdwa51JTsYSC1eDMUpjn'})
train.GetContentFile('train.ft.txt')

import numpy as np
import pandas as pd

def get_labels_and_text_from_file(file):
  labels = []
  texts = []
  openedFile = open(file, 'r')
  allLines = openedFile.read().split('\n')
  for line in allLines:
    if line != "" :
     labels.append(int(line[9]) - 1)
     texts.append(line[10:])
  openedFile.close()
  return np.array(labels), texts

train_labels, train_texts = get_labels_and_text_from_file('train.ft.txt')
test_labels, test_texts = get_labels_and_text_from_file('test.ft.txt')

import re
def text_processing(texts):
  processed_text = []
  #Pattern to recognize only letterns and numbers 1 and 2
  pattern = re.compile('[^a-z0-1\s]')
  for t in texts:
    #Transform all letters to lower case
    text_lower = t.lower()
    #Any other symbols are replaced with space
    transformed = re.sub(pattern, ' ', text_lower)
    processed_text.append(transformed)
  return processed_text

train_texts = text_processing(train_texts)
test_texts = text_processing(test_texts)

#Split the train set: 80% for train and 20% for validation
from sklearn.model_selection import train_test_split
train_texts, validation_texts, train_labels, validation_labels = train_test_split(train_texts, train_labels, test_size = 0.2)

import tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words = 12000)

#Updates internal vocabulary based on a list of texts
tokenizer.fit_on_texts(train_texts)

#Transforms each text in texts to a sequence of integers
train_texts = tokenizer.texts_to_sequences(train_texts)
validation_texts = tokenizer.texts_to_sequences(validation_texts)
test_texts = tokenizer.texts_to_sequences(test_texts)

def maxim_length(texts):
  maxim = 0
  for t in texts:
    if len(t) > maxim:
      maxim = len(t)
  return maxim

from tensorflow.keras.preprocessing.sequence import pad_sequences

#Transform all sequences into sequences of the same length
max_length = maxim_length(train_texts)
train_texts = pad_sequences(train_texts, maxlen = max_length)
validation_texts = pad_sequences(validation_texts, maxlen = max_length)
test_texts = pad_sequences(test_texts, maxlen = max_length)

#Using RNN
from tensorflow.python.keras import layers, models
def build_rnn_model():
  sequences= layers.Input(shape = (max_length, )) 
  embedded = layers.Embedding(12000, 64) (sequences)
  x = layers.CuDNNGRU(128, return_sequences=True) (embedded)
  x = layers.CuDNNGRU(128) (x)
  x = layers.Dense(32, activation='relu') (x)
  x = layers.Dense(100, activation='relu') (x)
  predictions = layers.Dense(1, activation='sigmoid') (x)
  model = models.Model(inputs = sequences, outputs = predictions)
  model.compile(
      optimizer = 'rmsprop',
      loss = 'binary_crossentropy',
      metrics = ['binary_accuracy']
  )
  return model

rnn_model = build_rnn_model()

rnn_model.fit(
    train_texts,
    train_labels,
    batch_size = 128,
    epochs = 1,
    validation_data = (validation_texts, validation_labels),
)

#Find the accuracy for the model
from sklearn.metrics import accuracy_score
p = rnn_model.predict(test_texts)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (p > 0.5))))

text1 = "Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing 'Who was that singing ?'"
text2 = "Long and boring: I've read this book with much expectation, it was very boring all through out the book"
example = []
text_p1 = text_processing(text1)
example.append(text_p1)
text_p2 = text_processing(text2)
example.append(text_p2)
example = tokenizer.texts_to_sequences(example)
example = pad_sequences(example, maxlen = max_length)
pred = rnn_model.predict(example)
print(pred)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (p > 0.5))))

#Using CNN
from tensorflow.python.keras import layers, models
def build_cnn_model():
  #Instantiate a Keras tensor
  sequences= layers.Input(shape = (max_length, )) 
  #Turns positive integers (indexes) into dense vectors of fixed size
  embedded = layers.Embedding(12000, 64) (sequences)
  #Convolution kernel is convoled with the layer to produce a tensor of outputs
  #(output_space, kernel_size, linear_unit_activation_function)
  x = layers.Conv1D(64, 3, activation='relu') (embedded)
  #Normalize and scale inputs or activations
  x = layers.BatchNormalization() (x)
  #Downsamples the input representation by taking the maximum value over the window
  x = layers.MaxPool1D(3) (x)
  x = layers.Conv1D(64, 5, activation='relu') (x)
  x = layers.BatchNormalization() (x)
  x = layers.MaxPool1D(5) (x)
  x = layers.Conv1D(64, 5, activation='relu') (x)
  #Downsamples the input representation by taking the maximum value over the time dimension
  x = layers.GlobalMaxPool1D() (x)
  x = layers.Flatten() (x)
  #First parameter represents the dimension of the output space
  x = layers.Dense(100, activation='relu') (x)

  #Sigmoid function: values <-5 => value close to 0; values >5 => values close to 1
  predictions = layers.Dense(1, activation='sigmoid') (x)

  model = models.Model(inputs = sequences, outputs = predictions)

  model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['binary_accuracy']
  )

  return model

cnn_model = build_cnn_model()

#Trains the model for a fixed number of iterations (epcohs) on a dataset
cnn_model.fit(
    train_texts,
    train_labels,
    batch_size = 128,
    epochs = 2,
    validation_data = (validation_texts, validation_labels),
)

#Find the accuracy for the model
from sklearn.metrics import accuracy_score
p = cnn_model.predict(test_texts)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (p > 0.5))))

text1 = "Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing 'Who was that singing ?'"
text2 = "Long and boring: I've read this book with much expectation, it was very boring all through out the book"
example = []
text_p1 = text_processing(text1)
example.append(text_p1)
text_p2 = text_processing(text2)
example.append(text_p2)
example = tokenizer.texts_to_sequences(example)
example = pad_sequences(example, maxlen = max_length)
pred = cnn_model.predict(example)
print(pred)
print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (p > 0.5))))